  \documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,draft]{hyperref}


%%% comment \ShowNotes out to remove all colored comments defined with \newcommand below %%%
\newcommand*{\ShowNotes}{}

\ifdefined\ShowNotes
  \newcommand{\colornote}[3]{{\color{#1}\bf{#2: #3}\normalfont}}
\else
  \newcommand{\colornote}[3]{}
\fi
% maybe requires 
\usepackage[usenames]{color}
\definecolor{darkred}{rgb}{0.7,0.1,0.1}
\definecolor{darkmagenta}{rgb}{0.1,0.7,0.1}
\definecolor{cyan}{rgb}{0.7,0.0,0.7}
\definecolor{dblue}{rgb}{0.2,0.2,0.8}
\definecolor{maroon}{rgb}{0.76,.13,.28}
\definecolor{burntorange}{rgb}{0.81,.33,0}

%\newcommand {\note}[1]{\colornote{maroon}{Note}{#1}}
\newcommand {\todo}[1]{\colornote{cyan}{TODO}{#1}}
\newcommand {\lihi}[1]{\colornote{magenta}{LZ}{#1}}
\newcommand {\ayellet}[1]{\colornote{blue}{AT}{#1}}
\newcommand {\nadav}[1]{\colornote{red}{NA}{#1}}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Partial Matching of 3D Shapes using Deformable Diversity}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
51
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
We propose a novel approach for the matching of partial deformable shapes in 3D. Inspired by recent advances in 2D template matching techniques, our method relies on the concept of deformable diversity similarity(DDIS), extends and adapts it from an image to the 3D shape domain, and leverages the distinct behavior of this framework in different scales to achieve shape correspondences. We evaluate this framework on the SHREC16 partial matching of deformable shapes and show state of the art performance in achieving sparse correspondences.
\ayellet{order of work: 1. Section 3 (Approach), 2. Section 4 (Similarity), Section 5 (Results), Section 2 (Related work), Section 1 (Introduction).
}
{\color{cyan}{\bf Currently only Section 3 is sort of done}}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Problem Definition
Shape correspondence is a fundamental and challenging problem in computer vision and graphics. It has usage in various applications such as transferring texture and animation. 
Shapes rarely, if ever manifest in only one pose. While rigid transformations between surfaces is a well researched topic with many adequate solutions, a more challenging problem arises when a shape is deformed non-rigidly, a case all too common for people, animals and objects.
Moreover, the shape acquisition process almost always lead to partiality of the scanned object. Occlusions arise from different angles of acquisition, which cause an object to occlude itself, or stem from other occluding objects. 
An additional type of difficulty which might be occur is topological noise, occurring when shapes touch pn another, thus making  sensors unable to seperate them.
All of these combined give rise to the challenging problem of partial correspondences, where a deformed and incomplete shape, possibly with topological changes, has to be matched with its full version. The goal of this paper is to deal with this challenging problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Previous Works
While in a rigid setting the problem can be solved by RANSAC and ICP like approaches\cite{rusu2009fast, holz2015registration}, extending these to non-rigid case produces mediocre results due to an underlying assumption of small deformations. 
%%%%Isometry based methods
Early methods specialized for the non-rigid problem focused on minimization of intrinsic metric distortion\cite{bronstein2006generalized,Torsello:2012:GAD:2354409.2354702} and regularity of parts\cite{Bronstein:2009:PSO:1553357.1553368,bronstein2008not}. These methods all contain with them a global assumption of isometry which holds only approximately, these tended to break down with it, and are also unable to handle extreme partiality. 
%%functional correspondences
Another family of method is based on functional correspondence. These methods model correspondences as a linear operator of a known nature between a space of functions on manifolds\cite{Ovsjanikov:2012:FMF:2185520.2185526}. These methods, originally designed for the full shape correspondence scenario have achieved state of the art results on various partial matching tasks in the recent years\cite{litany2017fully,vestner2017efficient,rodola2017partial}, and produce dense correspondence maps, but are not parallelizable, and their reliance on intrinsic metrics makes them invariant to symmetry. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Our key ideas
We take a different approach. We take advantage of the fact that while the isometric property tends to break over large distances, it usually holds approximately in limited environments. These also tend to suffer a lot less from boundary effects, especially when concentrated around the extremities of a shape. 

We can thus treat the problem of partial correspondences as matching of multiple templates, each smaller then the partial surface centered around shape landmarks. 

In addition, since point descriptors are known to be modified by partiality and deformations, instead of using them directly, we follow the approach of\cite{talmi2017template}(\textbf{DDIS}) which tackles template matching in 2D and use simple statistical assumptions on the nature of nearest neighbors between small patch descriptors, along with the assumption of an approximate conservation of distances in medium environments to obtain similarity scores between these partial shape templates.

We analyze the behavior of DDIS similarity in different scales and devise a multi scale scheme which leverages the advantages of each scale while masking their shortcomings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Our improvements
We show that using this approach, we are able to generate a set of sparse correspondences, which are less prone to symmetrical assignment than functional correspondence reliant methods, and are of superior quality on the SHREC16 Partial matching challenge\cite{cosmo2016shrec}. We then demonstrate how these sparse correspondences can be used as an input to existing functional correspondence algorithms to obtain dense correspondences or a higher quality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Our Contributions
In summary, our contributions are:
\begin{itemize}
	\item A non trivial extension of Deformable Diversity from 2 to 3 Dimensions.
	\item A modified DDIS similarity measure which is more well suited to handle matching of templates with a different number of points.
	\item An empirical analysis of DDIS behavior in different scales, leading to an improved multi-scale framework.
	\item A multi-template approach to partial matching of deformable shapes which can both produce state of the art sparse correspondences, and be used as an input to functional correspondence algorithms, significantly improving the results obtained by these.
\end{itemize}

The rest of the work is organized as follows: in section 2 we go over related works in the field of shape analysis. Section 3 introduces our Deformable Diversity framework for 3D shape matching. Experiments and results are given in section 4, and the conclusions are in section 5.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% section: Related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
\label{chap:related work}


\subsection{Matching Of Deformable Surfaces}
%%[APL15] [MDK16] [SPKS16][VLR17][OBCS12][KBB13][PBB13b][KBBV15][HWG14]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Shape Descriptors%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As a fundamental problem in computer graphics and vision, an extensive body of work have been done on the matching of surfaces.
A variety of shape descriptors have been devised for this task which can be roughly divided in to 2 families. 
Extrinsic ones, such as PFH\cite{rusu2008learning}, SHOT\cite{tombari2010unique} and FPFH\cite{rusu2009fast} which are usually calculated in euclidean space and are thus sensitive to non rigid deformations, but can discern between reflections and are also more robust to noise, topological artifacts and boundary effects.
On the other hand intrinsic features such as Heat\cite{bronstein2010scale} and Wave Kernel signatures\cite{aubry2011wave} are invariant under isometric transformations, but are very sensitive to partiality and are unable to discern between symmetric parts.
These have been commonly used to generate rough correspondences between surfaces and point clouds based on their similarity, but are noisy and offer little in terms of bijectivity and continuity of the solution. a measure of global consistency using these can be achieved by solving an energy minimization of the disimilarity matrices steming from an assignment, and the auction algorithm has been commonly employed for this purpose.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Distortion minimization%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Other methods use pairwise relations between points such as geodesic distances\cite{sahilliouglu2012minimum,sahilliouglu2012scale,sahillioǧlu2011coarse}, and search for a configuration which minimizes the distortions of these. These methods usually carry a high complexity, both due to calculating the pairwise relations, and the combinatorial configuration search, and are thus either obtain sparse matches\cite{sahilliouglu2012minimum,sahilliouglu2012scale,sahillioǧlu2011coarse} to alleviate this complexity, or used strategies such as coarse to fine solutions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Embedding Space%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Another common approach has been to embed the shapes into a different lower dimension "canonical"  space, this has been done by generalized MDS\cite{bronstein2006generalized}, an embedding into the mobius group\cite{lipman2009mobius}, or by representation in the LBO basis\cite{shtern2014matching}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Functional Correspondences%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 A notable family of works are derived from functional correspondences. Introduced at\cite{Ovsjanikov:2012:FMF:2185520.2185526,pokrass2013sparse,kovnatsky2015functional,vestner2017efficient} these assume that functions can be mapped from one manifold to another via a linear operator, finding this transfer operator allows to embed point in a space where the ICP method can obtain correspondences. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Learning%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Lately there has been a large body of works which employ learning methods such as Random Forests\cite{Rodola:2014:DNS:2679600.2679987} and deep learning architectures\cite{masci2016geometric,boscaini2016learning,monti2017geometric}. These show the promise of achieving state of the art performance, but require a lot of annotated data.

\subsection{Partial Matching of Deformable shapes}
The introduction of partiality adds complications which are not present in the full correspondence scenario. Spectral quantities change drastically, while geodesic paths disappear.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Rigid partial matching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For the rigid setup, the Iterative Closest Point(ICP)\cite{Aiger:2008:CSR:1360612.1360684} algorithm, preceded by initial alignment\cite{rusu2008towards} tackle partial matching successfully. Adapting this to the rigid setup however has proved to have limited success due to the alignment which is necessary, and thus is only fit for very small non-rigid deformation.

Early works which were designed with partial matching in mind\cite{bronstein2008not,bronstein2009partial} formulated an energy minimization problem over metric distortion and regularity of corresponding parts. Following works relaxed the regularity requirement by allowing for sparse correspondences\cite{Torsello:2012:GAD:2354409.2354702,rodola2013elastic}. 
Other works\cite{sahilliouglu2012scale,sahilliouglu2012minimum} minimized the distortion metric over the shape extremities by doing combinatorial search of least distortion matches and then densify them while employing a refining scheme in the process. 

In\cite{pokrass2013partial} a bag of words point-wise descriptors on a part in conjunction with a constraint on area similarity and the regularity of the boundary length to produce correspondence less matching parts without point to point correspondences by energy minimization.

Another line of works employ machine learning techniques to learn correspondences between manifolds. 
Recently \cite{rodola2017partial} had proven that partiality induces a slanted diagonal structure in the correspondence matrix and found the Laplacian eigenfunctions from each basis which induces this structure. Current state of the art\cite{litany2017fully} uses this notion in conjunction with joint diagnolization. The main drawback of this method, shared with other intrinsic methods, is its invariance to symmetries. 

\textbf{3D Shape Descriptors} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% section: 2D Shape Matching
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Template matching in 2D}
Template matching in 2D is a well researched topic. Similarly to 3D objects are going complex deformations of pose, and are only seen partially depending on the camera point of view. Recently a series of works which use a very simplistic framework based on the statistical properties of nearest neighbors in low level feature space had made good strides in tackling this complex task.

\textbf{Best Buddies Similarity}
Great strides had been achieved in the field of 2D template matching. Best Buddies Similarity\cite{dekel2015best} is a simple framework which employs a statistical assumption - if two regions $\mathcal{N}$,$\mathcal{M}$ contain the same template patches should maintain Bi Directional Similarity. 
That is - given a point $n_i\in\mathcal{N}$ and a corresponding point $m_i\in\mathcal{M}$ they should point too each other as nearest neighbors - that is if $NN_{\mathcal{M}}(n_i)=m_j$ then on a matching template we should expect $NN_{\mathcal{N}}(m_j)=n_i$. Solving for a matching template then amounts to finding the region which has the highest count of best buddies. This amazingly simple scheme has been show to be able to handle occlusions, missing parts and complex deformations of templates.

\textbf{Deformable Diversity Similarity}
Building upon the above work, \cite{talmi2017template}  relaxed the requirement for a best buddy relation, and added a requirement for spatial coherency.

The rather cumbersome best buddy relation has been relaxed to requiring only that the diversity of the set of nearest neighbors sets between corresponding templates should be high. This is actually prerequisite to a high best buddies similarity score and serves as a rough approximation of it. For this end diversity is formally defined as:
\begin{equation}
\underset{\mathcal{N}\rightarrow\mathcal{M}}{DIS}=c\cdot |\{n_i\in\mathcal{N}:\exists m_j\in\mathcal{M},NN(m_j,\mathcal{N})=n_i\}|
\end{equation}
where $|{.}|$ denotes group size and $c=1/min(|\mathcal{M}|,|\mathcal{N}|)$ is a normalization factor. Between non corresponding windows, indeed one should expect most points to have no real corresponding point, and thus be mapped to a very and remote nearest neighbors.  On the other hand, regions containing matching objects are drawn from the same distribution, thus the diversity of nearest neighbors should be high.
To accommodate this assumption not only did they rewarded high diversity of nearest neighbors, but also penalized mapping to the same patch. To this end, another, a negative diversity measure had been defined:
\begin{equation}
\kappa_{\mathcal{M}}(n_i)=|\{m\in\mathcal{M}:NN^a(m,\mathcal{N})=n_i\}|
\end{equation}
With $x_i^a$ denoting the appearance descriptor of point $x_i$.
Thus the contribution of a patch $m_j:NN^a(m_j,\mathcal{N})=n_i$ is $exp(1-\kappa_{\mathcal{M}}(n_i))$. 
An additional observation made has been that while non isometric deformations do occur, they should be restricted, small, in real objects. With  distance on the window pixel grid between 2 nearest neighbor points defined as $r_j=d(m_j^l,n_i^l)$ with $x_i^l$ denoting the location of $x_i$ on a grid, the final Deformable Diversity Similarity formulation becomes:
\begin{equation}
\underset{\mathcal{N}\rightarrow\mathcal{M}}{DDIS}=c\sum_{m_j\in\mathcal{M}} \frac{1}{1+r_j} \cdot exp(1-\kappa (NN^a(m_j,\mathcal{N})))
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% section: 3D Deformable Diversity Similarity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{General Approach}
\label{sec:approach}
\begin{figure*}[htb]
	\centering
	\includegraphics[width=1\textwidth]{figures/Birds_Flight.png}
	\caption{High level illustration of the DDIS Partial Correspondence pipe.}
\end{figure*}

Given two surfaces $\mathcal{M}$ and $\mathcal{N}$, the goal is to find the best match of  $\mathcal{N}$ within  $\mathcal{M}$.
In particular, we aim at defining a sparse set of point correspondences between the models. 
Our approach is based on three key ideas, which we describe hereafter.

%%%%%%%%%%%%%%%%%%%%Key Idea underlying the original DDIS : diversity
First, inspired by \cite{talmi2017template}, similarity is captured by two properties of the Nearest Neighbor field. 
(1) When the target and the template match, most target points have a unique NN-match in the template. 
This implies that the NN field should be highly diverse, in the sense that many different points in the template are being matched.
(2) Arbitrary matches typically imply a large deformation, whereas
\ayellet{finish this claim}
Therefore, Similarity should be based both on the diversity... and the consistency of the distances between the points.

% One key to our method is the assumption that while non rigid deformations and partiality leads to changes in the data, the underlying shape distributions from which the shapes on the surfaces are drawn should remain highly overlapping, and thus the data points drawn from these should be spread approximately uniformly, one inside the other, leading to a high diversity of nearest neighbors.
% %%%%%%%%%%%%%%%%%%%%%The idea underlying our improvement%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% However, since unlike in\cite{talmi2017template} we don't have the benefit of having a template and a query of the same size in the absence of a grid, and the space of geometric shapes is more constrained then that of RGBXY, one should not expect a near bijection. 
%%%%%%%%%%%%%%%%%%%%Key Idea underlying the original DDIS : low deformation%%%%%%%%%
% Another assumption is that of a geometrical coherency  - geodesic distances of corresponding pairs of points on a shape and a roughly isometrically deformed version of which should hold approximately leading us to use the assumption of low distortion.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Our added observation%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% However, the preservation of distances holds only approximately even for full template matching, and even less so in the presence of partiality and topological noise.
% It holds that the longer a distance between 2 points is, the more likely it is that a distortion had occurred on the geodesic path between them, either due to a non isometric deformation, partiality removing a piece of the geodesic path or topological noise introducing a shortcut.
% It follows from this that the smaller the pieces we are trying to match will be, the higher the likelihood of pairwise distances to be preserved should be.
Second, rather the performing the above similarity test on $\mathcal{N}$ as a whole, it is preferable to perform it on smaller set of sub-surfaces of it.
This is so not only since smaller sub-surfaces are more likely to exhibit consistent distances, but also since they are less likely to be matched to repeating patterns that lead to lower diversity.

Third, a multi-scale approach is beneficial, since matches of coarser scales are globally consistent but less accurate, whereas finer scales lead to better localization, \ayellet{NOT clear: but alone might lead to mappings to a similar point which lie in a different region.}

Therefore, our algorithm, which is illustrated in \ayellet{Figure XXX}, consists of the following steps.
\begin{enumerate}
    \item 
    \textbf{Pre-processing}.
    Shapes descriptors are calculated for both meshes and an approximate nearest neighbor field is computed for them.
    
    Many descriptors have been proposed in the literature\cite{rusu2008towards,tombari2010unique,Sun:2009:CPI:1735603.1735621} \ayellet{citations for three major ones}.
    We are using the \ayellet{ FPFH~\cite{rusu2009fast}}  due to being robust to small deformations and partiality of data, yet sensitive to symmetrical flips. \ayellet{why is this sensitivity important?}\nadav{I believe this is the reason we don't have as many symmetrical limbs assignments as competing methods.}
    \item
    {\bf Sampling \& corresponding subsurface.} 
    Inline with the second key idea, we aim at extracting a meaningful set of points, whose neighborhoods provide a good coverage of the surface. 
    To extract them, we start from the extremities, which are salient points.
    Then, we iteratively refine the set, adding more samples, which together cover more of the surface.
    
    \ayellet{will we define extremities later? if not, this is the place - copy from our old paper}\nadav{my current plan is to give a paragraph to the sampling scheme which will include this}
    
    Then, in an iterative manner, we choose a sample point \ayellet{how?}, and extract the closest unmarked vertex whose geodesic distance to this sample is larger than a pre-defined threshold. \ayellet{and what if it is close to another sample?}\nadav{It is guaranteed because of the marking process that we will not choose a sample close to one already in the set}
    \nadav{proposed rewrite of the above: Then, in an iterative manner, we choose the sample point whose distance to the set is the minimal for the set of points whose distance to the set is at least $R_{Sample}$}
    
    Once the set of representing samples is constructed, a sub-surfaces is extracted around each point, defined to be the geodesic disc of radius $R_T$ around it.
    \ayellet{do we guarantee that they are non-overlapping or we do not care?}\nadav{We indeed do not care. They are overlapping.We only put a constraint on the minimal distance between samples}
    
    \item
    {\bf Computing the similarity function.} 
    This is the core of our algorithm.
    For every sample point of $\mathcal{N}$, we
    % we define a geodesic disc around it and 
    compute a similarity score between it to all the points of  $\mathcal{M}$.
    \ayellet{is that correct or the opposite? - your answer is irrelevant. Look at the problem definition,  $\mathcal{N}$  $\mathcal{M}$ are surfaces.}\nadav{This is probably something we should go over briefly tomorrow, as the difference here conuses me.}
    %\nadav{reply: Yes - if we have N samples on the part and M vertices on the full model we will have M similarity scores for each of the N samples}
    
    The idea is to reward a NN field with high diversity and low deformation. %Unlike\cite{talmi2017template}, we have found it beneficial not to penalize points which share the same nearest neighbor, but rather only use the one which has the lowest deformation.
    A point that maximizes the similarity function that realizes this idea is considered the corresponding point.
    We will elaborate on how this is performed in Section~\ref{sec:similarity}.
    % \textbf{Refinement.} The goal of this stage is to achieve better correspondences based on simple global tests and constraints
    % \begin{itemize}
    % \item Correspondences which exhibit high distortion are determined are marked as not trusted.
    % \item For Correspondences which are deemed not trusted we search among the local maximas of the DDIS score a point which minimizes their distortion w.r.t to trusted point.
    % \end{itemize}
    \item
    \ayellet{Is there a stage that takes the point correspondences and creates surface correspondences?}
    \nadav{It is currently defined as optional - and it is the use of these sparse correspondences as an input to the algorithm of \cite{litany2017fully} to produce a dense set of correspondences.}
    \ayellet{I do not understand: You never extract a sub-surface??}\nadav{no. in the end the result of our algorithms are point wise correspondences. Either sparse, or dense.}
\end{enumerate}
The latter two steps are performed in a multi-scale approach. 
To do it, we define a set of $N$ scales, which define geodesic radii $R_{T_1}>...>R_{T_N}$, \ayellet{which are the pre-defined thresholds of Step 2}\nadav{No, they are not. The density of sampling radius used for step 2 is not the same as the radius we use to construct our geodesic discs}.
The larger the radius, \ayellet{the ....}
\ayellet{ I do not understand anything you say below. Do you use the results in one scale to calculate the other. How is Step 3 influenced?}\nadav{At step 3 we calculate k similarity scores for each pair of points - one for each geodesic disc radius $R_{T_k}$}
\nadav{We then move from a coarse to fine scale by iteratively finding the maxima of similarity at scale $R_{T_k}$, and use to define a valid set of points from which we pick the maxima at scale $R_{T{k+1}}$ by constraining them to lie in a geodesic disc of radius $R_{T_k}$ around it. We repeat this process and determine the final correspondence at scale $N$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% section: Similarity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Similarity}
\label{sec:similarity}

%First paragraph: problem definition
Given $\mathcal{M}$ and $\mathcal{N}$ (Section~\ref{sec:approach}) and a vertex $v \in  \mathcal{N}$ \ayellet{yes?, not the opposite?}, the goal is to find the most similar vertex on $\mathcal{M}$.
We would like the similarity to be oblivious to non-rigid transformation, to the resolutions of the meshes, to noise, and to the partiality of the data.

\ayellet{

Second paragraph: Key idea, first in laymen language.
We are inspired by...., but found that the method there is insufficient due to the following reasons.

Third paragraph: the mathematical definition that realizes the key idea

Fourth paragraph: outline of similarity computation

Then, paragraphs/subsections explaining the different stages
}


%Second paragraph: Key idea, first in laymen language.
\nadav{proposed text: 

We are inspired by the principles introduced in\cite{talmi2017template}, which rely on the observation that NN field mapping points from a matching target to a template should have a high diversity of unique matches, and a preservation of distances between said matches, but had found that the method there is insufficient for our task due to the following reasons.

First, the way in which 3D data is organized forces a comparison between shapes with an asymmetrical size of data. This introduces different biases, such as penalizing pieces with repeating patterns, which occur more frequently when the search window contains more data points than the template. This problem is escalated even further by partiality as symmetrically flipped shapes on a full model will be mapped to the same points.

In addition the absence of a simple grid makes the relation between locations of two points on different shapes to become not well defined.

%first idea
We address the former issue first, by adopting an elitist strategy: we count only the contribution of the nearest neighbor for which distances are preserved the best and effectively ignoring the rest.

%2nd idea
In addition, we compare small segments of the part, instead of the whole, as this decreases the chance for pieces to be of a vastly asymmetrical area.

%3rd idea
In order to measure deformation we choose a reference point and measure all distances in relation to it.}

%Third paragraph: the mathematical definition that realizes the key idea
\nadav{proposed text:

Let points $n_i,m_j$ represent surface regions of the template and target respectively, with ${n^l}_i,{m^l}_j$ denoting their location, and ${n^S}_i,{m^S}_j$ denoting their shape, described by some given vector.
Our goal is to measure the similarity between two sets of points, the template points $\mathcal{N}^{R_T}_{c}=\{n_k\}\in\mathcal{N},d({n^l}_k,{n^l}_c)<R_T$ ,and the target points ${\mathcal{M}^{R_T}_{c'}}=\{m_k'\}\in\mathcal{N},d({m^l}_k,{m^l}_c')<R_T$, where $d$ is a given spatial distance function and ${n^l}_c,{m^l}_c'$ are given reference locations on $\mathcal{N},\mathcal{M}$ respectively. 
We require finding the NN in $\mathcal{N}$ for every point $m\in\mathcal{M}$ s.t. $NN(m,\mathcal{N})={\underset{n\in\mathcal{N}}{\arg\min\ }} d(n^S,m^S)$ for some given distance function defined on their shape function space. The property of diversity can be measured intuitively by counting the number of unique NNs. 
Thus, same as in \cite{talmi2017template} we define Diversity as:
\begin{equation}
\underset{{\mathcal{M}^{R_T}_{c'}}\rightarrow{\mathcal{N}^{R_T}_{c}}}{DIS}=|\{n_i\in{\mathcal{N}^{R_T}_{c}}:\exists m_j\in{\mathcal{M}^{R_T}_{c'}},NN(m_j,\mathcal{N})=n_i\}|
\end{equation}
where $|\{.\}|$ denotes group size. 

In order to take into account our second desired property of pairwise distance consistency, for each point $n\in{\mathcal{N}^{R_T}_{c}}$ we search for a point $m\in{\mathcal{M}^{R_T}_{c'}}$ such that $NN(m,\mathcal{N})=n$ and whose distance to the source $d(m^l,m^l_c')$ is the most similar to $d(n^l,{n^l}_c)$. 
We define the deformation induced by $n_i$ as 
\begin{equation}
r_i = min(|d(m_j,c')-d(n_i,c)|),\\
\end{equation}
where $m_j\in {\mathcal{M}^{R_T}_{c'}}, NN^S(m_j,\mathcal{N})=n_i$. If $n_i$ has no point who points to it as its nearest neighbor in ${\mathcal{M}^{R_T}_{c'}}$ then $r_i=\infty$.

We then use this induced deformation weigh it's contribution, and finally we define our Deformable Diversity Similarity by the equation:
\begin{equation}
DDIS({\mathcal{N}^{R_T}_{c}},{\mathcal{M}^{R_T}_{c'}},\gamma)=\sum_{n_i\in {\mathcal{N}^{R_T}_{c}}}\frac{1}{1+r_i}
\end{equation}
Note that when the deformation is 0, that is $r_i=0 \forall n_i$ DDIS becomes identical to the diversity score, while when $r_i\rightarrow\infty$, the contribution of a point having a nearest neighbor to the similarity score will become 0, irrespective to if it has multiple nearest neighbors. Thus this formulation indeed captures the properties we have defined above, while disregarding distractors altogether.
} 

%Fourth paragraph: outline of similarity computation
\nadav{outline of similarity computation: proposed text: 
%preprocessing
To calculate our similarity we first compute shape descriptors for all the point on $\mathcal{N},\mathcal{M}$ and use Approximate Nearest Neighbor in the descriptor space to infer the Nearest Neighbor Field.
%template construction
We then collect a neighborhood $\mathcal{N}^{R_T}_{c}$ of a given Radius $R_T$ around a desired source point $n_c$ and compute the distance of all points which lie within this neighborhood to the source $n_c$.
%similarity calculation
We repeat the same process for a candidate point $m_c\in\mathcal{M}$ to create ${\mathcal{M}^{R_T}_{c'}}$.
We traverse all the points in ${\mathcal{M}^{R_T}_{c'}}$ and check for each one if it's distance to its center is the most similar to the distance of its nearest neighbor in ${\mathcal{N}_c}^{R_T}$ and update the contribution of $n_i$ to the similarity score accordingly.
Finally we go over all points $n\in\mathcal{N}^{R_T}_{c}$ and sum their contribution to obtain a similarity score.
}

%paragraphs/subsections explaining the different stages
\nadav{subsections explaining the different stages, proposed text: 
\subsection{Shapes Preprocessing}
Before we can calculate similarity between our shapes some preprocessing of the shapes is necessary. 
We calculate a characteristic length on the full part denoted $R_\mathcal{M}=\sqrt{Area(\mathcal{M})}$. We also compute vertex-wise normals using the face normal weighting scheme introduced in\cite{max1999weights}. 
\subsection{Nearest Neighbor Field calculation}
To infer our nearest neighbor field we first calculate FPFH\cite{rusu2009fast} point descriptors for every point on the template $\mathcal{N}$ and the query shape $\mathcal{M}$. We set a neighborhood of $r_F=\alpha\cdot R_\mathcal{M}$ and around every point, where $\alpha$ is a tunable parameter for the calculation of FPFH. We then calculate approximate nearest neighbors in FPFH space using FLANN with a $\chi^2$ distance measure to obtain a nearest neighbor mapping $\underset{\mathcal{M}\rightarrow\mathcal{N}}{NNF}=\{NN(m_i,\mathcal{N})\}_{i=1}^{|\mathcal{M}|}$
\subsection{Template construction}
In order to calculate our similarity measure, another necessary step is to have 2 pieces of roughly the same volume, and a way to compare the locations of points on them. In the absence of a simple grid, this is done by picking a reference point $x$ as the center and collecting all the points in a surrounding neighborhood of a defined radius $R_T=\beta R_\mathcal{M}$, where beta is a tunable parameter. We calculate the distance from the reference point to every point on the selected piece. This is done once for a chosen point $n_c\in\mathcal{N}$ to construct $\mathcal{N}^{R_T}_{c}$.
\subsection{Similarity Calculation}
We iterate over all points $m_{c'}\in\mathcal{M}$ and construct ${\mathcal{M}^{R_T}_{c'}}$. Inside the piece we iterate over every point $m_j\in{\mathcal{M}^{R_T}_{c'}}$ and calculate $r_i=\frac{|(d({m^l}_i,{m^l}_c')-(NN(m_i,\mathcal{N})^l,{n^l}_c|}{\gamma\cdot R_\mathcal{M}}$, where if $NN(m_j,\mathcal{N})$ is outside ${\mathcal{N}_c}^{R_T}$ this distance is disregarded. Notice the normalization by $\gamma R_\mathcal{M}$ here which is done to make the distances scale invariant, while $\gamma$ is a tunable parameter which should correlate to the expected distortion. We find for each $n_i\in{\mathcal{N}_c}^{R_T}$ the distance of the point who is pointing to it as a nearest neighbor and has the most similar distance to its reference point. If $n_i$ has no point who point to it as its nearest neighbor then $r_i=\infty$. Finally we calculate The similarity between the pieces by our similarity function in Eq. XX.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% section: Deformable Diversity Similarity in 3D
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deformable Diversity Similarity in 3D}
\begin{figure}[htb]
	
	\includegraphics[width=0.5\textwidth]{figures/DDIS2.png}
	\caption{Illustration of the Nearest Neighbor Field between different shapes. 
		Geodesic distances are color coded by the jet scheme, with on surface color denoting distance from source, and line colors the deviation. 
		You can notice that on identical pieces, and even on deformed matching pieces there are multiple diverse matches, most of which are colored in blue to indicated very similar distances from the source point, whereas on different pieces most lines map to very few points and a lot of yellow lines (high deformation) exist}
\end{figure}

\nadav{Old text:The nature of 3D data gives rise to unique problems which do not occur in the 2D scenario. Data is distributed in space both sparsely and with varying densities - the amount of data points occupying a given volume can vary drastically.
A second problem arises from the absence of a regular grid. These problems require different definitions for key components to the 2D deformable diversity formulation. For this work we chose the image patch to be replaced by a neighborhood which is required to calculate a selected shape descriptor, usually a small sphere in euclidean space or a surface patch with a radius $r_F$. 
The search window of a template is defined as a geodesic disc around the query point, with a radius denoted by $R_T$. The pixel grid distance is replaced by either a euclidean distance $d_{Euc}(x^l,y^l)$(in the case of point clouds) or geodesic distance $d_{Geo}(x^l,y^l)$(for surface meshes).
Given these DDIS  between shape parts $\mathcal{M}_{x,R_T}$ and $\mathcal{N}_{y,R_T}$ can be naively formulated as:
\begin{equation}
DDIS=c\cdot\sum_{m_j\in\mathcal{M}_{x,R_T}}\frac{exp(1-\kappa(NN^S(m_j,\mathcal{N}_{y,R_T}))}{1+r_j}
\end{equation}
where $\mathcal{M}_{x,R_T}$ and $\mathcal{N}_{y,R_T}$ are the shape parts in a radius $R_T$ surrounding the points $x\in \mathcal{M}$ and $y \in \mathcal{N}$ respectively, $r_j$ is the induced deformation
\begin{equation} 
r_j=\frac{|d(m_j^l,m_x^l)-d(NN^S(m_j,\mathcal{N}_{y,R})^l,n_y^l)|}{\gamma\cdot R_T}
\end{equation}
,where $\gamma$ is a selected fraction and $c$ is a normalization coefficient $c=1/min{|\mathcal{N}_{y,R_T}|,|\mathcal{M}_{x,R_T}|}$.

However, we wouldn't like to penalize our similarity score in case of repeating patterns or symmetrical shapes which have both symmetries in the template search window. Intuitively and empirically the exponent is too harsh and indeed unnecessary as both deformity and diversity will attenuate the score in case of multiple nearest neighbors. On the other hand, we wouldn't want to reward far correspondences at all because they are unlikely to originate from a corresponding patch. 



To account for this the following formulation has been found to work better: given a point $n_i\subset \mathcal{N}_{y,R}$ has a set of nearest neighbors in descriptor space on a geodesic disc 
$\mathcal{M}_{n_i}=\{m_j\in\mathcal{M}_{j,R_T}:NN^S(m_j,N_{y,R_T})=n_i\}$ 
, we define 
$m_i' = \underset{m_j\in\mathcal{M}_{n_i}}{argmin}\space (r_j)$
and $r_i'$ the minimal distortion distance, we add only the contribution of this point to the similarity score which then becomes}
\begin{equation}
DDIS(N_{y,R_T},\mathcal{M}_{x,R_T},\gamma)=\sum_{m_i'}\frac{1}{1+r_i'}
\end{equation}
{\color{red} explain why -- can we see it visually on the same example?}{\color{magenta} added both qualitative(visual, and quantitative example}\\
\begin{figure}[htb]
	\includegraphics[width=0.5\textwidth]{figures/DDISvsWDIS.png}
	\caption{Qualitative and quantitative illustration of the improvement introduces by our formulation. On the top left the Nearest neighbor mapping between the part and its corresponding ground truth geodesic disc. As can be seen low distortion is occurring between the true corresponding points, but the symmetrical part which is not included on the partial model maps to the same points - thus attenuating the score originating from these. One the bottom left we see the piece chosen by the original DDIS formulation. The arrows with low distortion seldom have other points which are their nearest neighbors and thus achieves a better score without our modification. The cumulative error curves show an improvement of ~5 percent on the training set using our formulation.}
\end{figure}
This equation still promotes both diversity and low deformations, but is less biased against surfaces which are either symmetrical, or exhibit repeating patterns and achieves a considerable improvement over the original formulation, as can be seen in Fig. XX.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% section: DDIS Correspondence between a Point and a Surface
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DDIS Template Matching}
{\color{red} Goal of the algorithm}
In this section we go over the flow of template matching of 3D shapes using DDIS, the solution of which constitutes the core of our partial shape matching. Given a template $\mathcal{N}_{y,R_T}$ with a reference point $n_y$ as it's center and a maximal distance $R_T$, we aim to find on a model $\mathcal{M}$ which has a deformed version of it, the corresponding surface piece $\mathcal{M}_{y*,R_T}$ and it's center $m_y*$. The solution is obtained by finding the point on $\mathcal{M}$ whose surrounding geodesic disc maximizes the above mentioned DDIS measure.
{\color{red}key ideas}

{\color{red} Overview}
We'll first give an overview, and then give an extended description of each of each stage. 

We start by calculating the normals for $\mathcal{M}$ and $\mathcal{N}$.
We than calculate local point descriptors for each patch of some neighborhood around the points in each mesh(For our purpose FPFH seemed to work the best of our tested descriptors). 
Having calculated these, we calculate a nearest neighbor field by finding for each patch in $\mathcal{M}$ it's Nearest Neighbor in $\mathcal{N}$. 
We now find the distance of every point $n\in \mathcal{N}$ to the desired point $y\in \mathcal{N}$ for a desired neighborhood $R_T$.
We now go over every point $x\in mathcal{M}$. For each we extract the geodesic disc $\mathcal{M}_{x,R_T}$ around it.
We take notice that while the above stages are done here in the context of template matching for one template, when matching multiple templates all of the above calculations have to be done only once between the shapes, with the exception of geodesic distance field extraction for the template itself. 
Finally we calculate DDIS for this disc with $\mathcal{N}_{y,R}$ which has $y$ as it's center. Having done that for every point, the point $y*\in \mathcal{M}$  which gets the maximal DDIS Score is deemed the corresponding point to $y$.


\textbf{Point Normal Estimation}
There are various schemes for estimating point normals given a triangulated mesh surface. We have picked the one which is available in the standard PCL. Given a vertex $p_i$ on a triangulated mesh $\mathcal{X}$ and it's associated polygons $\{A_j\}_{j=1}^k$ and their normals $N_{A_j}$ the point normal $N_i = \sum_{j=1}^k{|A_j|\cdot N_{A_j}}$

\textbf{Local Patch Descriptor}
%%%%%%%%%%%%%Defining an equivalent to a 2D patch#########
DDIS as defined by \cite{talmi2017template} uses patch descriptors as low level features for their similarity measure. While a patch in an image can by defined by the images grid no such grid exists on 3D point clouds and meshes, where density of data points can vary. Thus a patch has to be defined by some geometric measure. While the more robust way to define it would be using geodesic distance, since we are talking a small environment around a point on the mesh we have found that for practical purposes a patch in a defined euclidean radius $r_F$ around a point serves well enough. We pick this radius in the following way: given the full surface mesh $\mathcal{M}$ we find it's equivalent of a diameter $D_\mathcal{M} = \sqrt{Area(\mathcal{M})}$, and tune a parameter $\alpha$ to obtain $r_{F} = \frac{\alpha}*D_\mathcal{M}$
%%%%%%%%%%%%%%Descriptor selection#######################
A lot of local shape descriptors have been used successfully in 3D shape analysis. We have tested the following descriptors: PFH\cite{rusu2008learning} SHOT\cite{tombari2010unique},HKS\cite{Sun:2009:CPI:1735603.1735621},SIHKS\cite{bronstein2010scale},ROPS\cite{guo2013rotational} and FPFH\cite{rusu2009fast}. Out of these FPFH has achieved the best performance, and thus the descriptor for the local patch has been chosen to be FPFH. 

\textbf{Nearest Neighbor Field}
As an intermediate stage towards the calculation of Deformable Diversity Similarity measure, the calculation of the nearest neighbor field(will be abbreviated as $NNF$) needs to be calculated. Thus for every patch $m\in \mathcal{M}$ we have to find the patch on the template $n\in\mathcal{n}$ which resembles it the most. For $FPFH, NN^S(m_j,\mathcal{N})$ is defined $NN^S (m_j,\mathcal{N}) \equiv \underset{i}{argmin}\chi^2(FPFH(m_j],FPFH(n_i))$ and the Nearest Neighbor Field is the set of all these correspondences.

\textbf{DDIS calculation}
For every point in $m_x\in\mathcal{M}$ we then extract a geodesic disk $\mathcal{M}_{x,R}$ with a radius $R_T$ around it and calculate deformable diversity score for it. The point which maximizes the similarity score gives us a correspondence $(y,y*)\in \mathcal{N}\times\mathcal{M}$.
%\\Since as we will show in the results section, imperfections in the isometry assumption lead to considerable localization errors, we move to a multiple template matching framework using DDIS, as will be described in the next section. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% section: DDIS Sparse Correspondence
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DDIS Sparse Correspondences}
A key takeaway from experimenting with DDIS as a template matching algorithm for partial matching has been that isometry does not hold, at least not globally.It does however, hold pretty well locally, especially at extremities. To this end we devise  multiple template matching framework for Partial correspondences of deformable shapes. We first obtain landmarks $F_init=\{f\}_i$ as described in[]. 
We then employ a simple sampling scheme which ensures a good quasi uniform covering of the surface.
Finally, we for each sample point we extract a geodesic disc, and find its counterpart on $\mathcal{M}$ which maximizes DDIS. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% subsection: Mesh Sampling
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Landmark Extraction}
We follow the work of\cite{katz2005mesh} to obtain mesh extremities. Given a shape the work employs the following framework to extract it's extremities. A point is detected as an extremity if it fulfills 2 conditions: - it's sum of geodesic distances is a local extrema, formally, for $v\in S$ ,where $S$ is a surface mesh, we define the set of points with a direct edge to it as $N_v$, the point is a critical point if :
\begin{equation}
\sum_{v_i\in S}d_{geo}(v,v_i)>\sum_{v_i\in S}d_{geo}(v_n,v_i), \forall v_n\in N_v
\end{equation}
An additional requirement for it to be an extremities is for it to lie on the convex hull of the shape's MDS. In this work we have dropped the last condition, but chose $N_v*$ - a neighborhood of $0.03\cdot \sqrt{Area(S)}$. This stage gives us the initial set of sample denoted $S_0$

\textbf{Mesh sampling}
We employ a sampling scheme very reminiscent of the one employed in\cite{sahillioǧlu2011coarse}. In each stage $k$, we take the set of samples $S_{k-1}$ and mark all the points in a radius $R_S$ around them. From the set of remaining point we choose the one whose distance to the set $S_{k-1}$ is minimal to create the set $S_k$. We repeat this process until all the points are marked.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% subsection: Landmark Template Matching
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[h]
	\caption{3DIS Sparse Correspondences}
	\begin{algorithmic}
		\Procedure{DDIS Correspondence}{$\mathcal{M},\mathcal{N},\alpha,\beta,\gamma$}\Comment{Returns point correspondence for critical points on $\mathcal{N}$}
		\State{$r_F\leftarrow \alpha /100 \cdot \sqrt{Area(\mathcal{M})}$}
		\State{$R_{thresh}\leftarrow \beta /100 \cdot \sqrt{Area(\mathcal{M})}$}
		\State{$N_\mathcal{M}\leftarrow ComputeNormals(\mathcal{M})$}
		\State{$N_\mathcal{N}\leftarrow ComputeNormals(\mathcal{N})$}
		\State{$F_\mathcal{M}\leftarrow FPFH(\mathcal{M},N_\mathcal{M},r_F)$}
		\State{$F_\mathcal{N}\leftarrow FPFH(\mathcal{N},N_\mathcal{N},r_F)$}
		\State{$NNF_{\mathcal{M}\rightarrow \mathcal{N}}\leftarrow ANN(F_\mathcal{M},F_\mathcal{N})$}
		\State{$\mathcal{N}_c=\{n:\sum_{n_i\in\mathcal{N}}d_{geo}(n,n_i) > \sum_{n_i\in\mathcal{N}}d_{geo}(n_N,n_i), \forall n_N:d_{geo}(n,n_N)<0.03\cdot \sqrt{Area(\mathcal{M})}\}$}
		\For{$n_y\in \mathcal{N}_c$} \Comment{DDIS calculation Loop}
		\State{$m_c*\leftarrow DDIS\_Correspondence(\mathcal{M},n_y,\mathcal{N},\alpha,\beta,\gamma)$} 
		\EndFor
		\State{return $\mathcal{M}_c\times\mathcal{N}_c=
			\{m_c*,n_c\}$}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\textbf{Landmark Template Matching}
For this end, we create a template for each landmark point - we collect all surface point in a surrounding geodesic disc of radius $R_T=\beta\cdot\sqrt{area(\mathcal{M}}$. While it might seem natural to calculate a different nearest neighbor field for each landmark template it has been empirically found that using the global nearest neighbor field gives much better results. This is hypothesized to occur due to the fact that this nearest neighbor field encodes global information when obtained this way and might  eliminates local distractors.

Each landmark template is compared to all surface parts of a similar $R_T$ on $\mathcal{M}$ to obtain correspondences.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% section: Multi-Scale DDIS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cascaded Multi-Scale DDIS}
The observation of the effects of the choice of $\beta$ and the trade-off between finer localization and avoidance of global errors naturally leads to the adoption of a multi scale framework. We calculate $DDIS$ score for multiple $beta$ values, and use the location obtained with a large beta to select a narrow environment in which we look for the maximum of DDIS with a smaller value $beta$, thus using the larger scale to get a rough global location, and the smaller scale to fit it into a more exact location. While this might be done at multiple custom scales we have found that the triplet $\beta ,2\cdot beta/3\beta/3$ works well.
\begin{figure}[htb]
	\includegraphics[width=0.5\textwidth]{figures/MultiScaleDis.png}
	\caption{Illustration of the multi scale framework. The magenta grid marks the area chosen by the previous scale to be valid. It can be seen that wrong maxima in lower scales are ignored due to this process}
\end{figure}

\begin{figure*}
	\centering
	\includegraphics[width=1\textwidth]{figures/AblationStudyThreshold.png}
	\caption{Effect of the $\beta$ parameter on the results: it can be seen that a smaller beta promotes better localization in a small neighborhood, while higher values of $\beta$ lead to more local errors but are more robust to global errors. It can be seen that the multi-scale cascade achieves better results both locally and globally.}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% section: Greedy outlier rejection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Greedy Outlier Fix}
The results obtained by the pipeline, while empirically already producing matches of superior quality to state of the art algorithms in the field nevertheless have no explicit requirement on generating a coherent set of matches. We thus as a post processing step identify outliers and replace them with more suitable correspondences. 
Inspired by distortion minimization methods such as\cite{sahillioǧlu2011coarse} we pick the distortion of a correspondence as a measure of its coherency to the set. For a correspondence $(y,x)\in \mathcal{N}\times\mathcal{M}$ the distortion as the mean of it deviation of relative distance to all other points in the set. Given a set of correspondences $P={p_i=(y_i,x_i)\in \mathcal{N}\times\mathcal{M}}$ the distortion is defined by the equation
\begin{equation}
	D(p_i) = \frac{1}{|P|-1}\sum_{j \neq i}{\rho(p_i,p_j)}
\end{equation}
where $\rho$ is the individual relative distortion induced by 2 correspondences
\begin{equation}
\rho(p_i,p_j) = max(\frac{d(y_i,y_j)}{d(x_i,x_j)},\frac{d(x_i,x_j)}{d(y_i,y_j)})
\end{equation}
where $d$ is the geodesic distance between 2 points.
We first filter out outliers by thresholding matches whose distortion is larger then 1.2 of the mean distortion, and then greedily add from the set of DDIS maximas the point which minimizes this distortion.
\subsection{Densification}
If necessary, the sparse correspondences produced by our algorithm can then be passed as an input for the method of \cite{litany2017fully} to produce dense matches. This is done in a manner identical to the refinement step which is already used in the method, and improves its results.

\section{implementation}
Our code which produces the sparse correspondences is implemented entirely in C++ using the Point Cloud Library. For geodesics we have used  Fast Marching Geodesics adapted from the code published by Ron Kimmel to run in parallel on multiple cores and in conjunction with PCL.
\section{Experiments and results}\label{section:results}

In this section we will briefly go over the experiments performed and their results. We'll introduce the datasets, detail our experiments and their results
\subsection{Experimental Results}
In this section we will briefly go over the available Datasets
\textbf{SHREC 2016 Partial Correspondence}  
The SHREC partial matching dataset\cite{cosmo2016shrec} consists of 8 base, neutral pose models: cat, centaur, dog, horse, wolf, and 3 humans – 2 males, and 1 female, each containing 10,000 vertices except for the wolf which contains only ~4,500 vertices. Each basic model has corresponding deformed partial shapes obtained either by cutting the shape with a plane or by adding holes on a deformed shape. The set has been divided into train and test sets. The train set is composed of 15 cuts for each base models totaling 120 models,and 10 holed shapes for each model for which ground truth point to polygon correspondences has been provided in barycentric coordinates. The test set is composed of additional 200 cuts and 200 holed shapes. We have tuned our parameters only on the 120 pairs of cuts.


\subsection{Error Metrics}
The output of partial matching algorithms (as defined in\cite{cosmo2016shrec}) are sub-vertex point-to-point correspondences between partial shapes.
For all experiments we use the standard practice of not penalizing symmetric solutions. Quality is measured according to the Princeton benchmark protocol \cite{kim2011blended}. For a pair of points $(x,y)\in \mathcal{N}\times \mathcal{M}$ between the full object $\mathcal{M}$ and the partial shape $\mathcal{N}$ produced by an algorithm, where $(x,y^*)$ is the ground truth correspondence the inaccuracy is measured by. We plot curves showing the fraction of correspondences whose errors are below a threshold of the normalized geodesic error
\begin{equation}
\varepsilon(x)=\frac{d_{\mathcal{M}}(y,y^*)}{\sqrt{area(\mathcal{M})}}
\end{equation}
where $d_{\mathcal{M}}(y,y^*)$ is the geodesic distance on $\mathcal{M}$, and has units of normalized length on $\mathcal{M}$. For dense correspondences over a dataset, $\varepsilon(x)$ is averaged over all matching instances.
\subsubsection{Central Points Localization}
In this experiment we have chosen for each Template mesh the center point $c_T$ and tried to match it to a point on the object using DDIS. Experiments have been done using FPFH, PFH and SHOT as patch descriptors with patch radiuses of $[2,3,4,5]$, the results of the opimal parameter for each descriptor are illustrated in fig. and visualizations of similarity maps of cuts are provided in fig. . It can be seen that good localization is obtained for points on a smooth surface, under high partiality conditions and strong deformations. Bad matches occur when a matched point resides on a heavily deformed patch, and when salient anchor points are deformed or cut.  Analysis of these results shows a drift in localization occurs when salient features are divided by strong unisometric deformations which serve as the motivation for the multiple template matching framework.

\begin{figure*}[htb]
	\centering

	\includegraphics[width=1\textwidth]{figures/ROCfigure.png}
	\caption{Comparison between descriptors: we show curves for the minimal distance of the top results. a noticeable addition occurs when adding the 2nd best match}
\end{figure*}

\subsection{Sparse Correspondences on the SHREC16 Test set}
In this experiment we have tested the performance of DDIS in producing sparse correspondences on the SHREC16 Partial Matching of Deformable Shapes competition. 
We had tuned our parameters on the SHREC16 training dataset using only the cuts part of it. The best results had been produced using FPFH with $r_F = 0.03\sqrt{Area(\mathcal{M}}$, and a piece size radii of $R_T=[0.6,0.4,0.2]\sqrt{Area(\mathcal{M})}$. For Geodesic distances we have found the fast marching algorithm to work the fastest, while giving the lowest error w.r.t. to exact geodesics. For a 10,000 vertices mesh it takes 60s to produce a full distance matrix, Though it should be noted this algorithm has a more efficient GPU implementation, and parallelization on a core brought the run time to ~12s with 6 threads. FPFH and Nearest Neighbor field takes 2s, and similarity between 2 pieces of ~10000 vertices each takes XXs on average, running on 6 threads of i7-2700k. Unlike optimization based algorithms this is highly parallelizable.
We achieve results comparable to the state of the art \cite{litany2017fully} quality wise, even though sparser in nature on both the Cuts and the Holes datasets, Where a particularly impressive result is reported on the Holes dataset, which can then be expanded without a loss of quality by feeding these to the FSPM\cite{litany2017fully} as input instead of low level shape descriptors.

\begin{figure*}[htb]
	\centering

	\includegraphics[width=1\textwidth]{figures/ROCSHREC16.png}
	\caption{comparison with other state of the art algorithms - it can be seen that although sparse in nature, the correspondence obtained by DDIS are much more accurate than the other methods. 
		A separate analysis has been done for correspondences which include boundary points, which tend to be more noisy, and internal points which are more sparse}
\end{figure*}

\begin{table}[h]
	\centering
	\begin{tabular}{c  c  c  c  c  c  c c} 
		\hline
		& PFM & RF & IM & EN & GT & DDIS  \\ \hline
		cuts & dense & dense & 61.3 & 87.8 & 51.0 & 132.2\\ \hline
		holes & dense & dense & 78.2 & 112.6 & 76.4 & 77.3 \\ \hline
		
	\end{tabular}
	\caption{mean number of correspondence obtained by the algorithms in the SHREC 16 competiton and our algorithm. Note that our algorithm can combine wi}
	\label{table:1}
\end{table}

\begin{figure*}[htb]
	\centering

	\includegraphics[width=1\textwidth]{figures/success_1}
	\caption{Good correspondences obtained by our method}
\end{figure*}


\begin{figure*}[htb]
	\centering
	\includegraphics[width=1\textwidth]{figures/fail2.png}
	\caption{Some notable failure cases}
\end{figure*}
{\small
	\bibliographystyle{ieee}
	\bibliography{egbib}
}
\end{document}